{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIT-PCBA \n",
    "this dataset is a proposed \"more difficult\" test than DUD-E. I used a 75/25 (sklearn default) stratified split. The dataset is heavily imbalanced, like DUD-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "sys.path.insert(0, \"/g/g13/jones289/workspace/hd-cuda-master\")\n",
    "# print(sys.path)\n",
    "import hdpy\n",
    "import hdpy.ecfp\n",
    "# from hdpy.analysis import load_pkl\n",
    "from hdpy.metrics import compute_enrichment_factor\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.rc('axes', labelsize=17)\n",
    "plt.rc('figure', titlesize=20)\n",
    "\n",
    "\n",
    "# SEED=125\n",
    "SEED=5\n",
    "# SEED=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: visualize the learning curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = Path(f\"/p/vast1/jones289/hd_results/{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_color_pal = sns.color_palette(\"Greens\", 10)\n",
    "blue_color_pal = sns.color_palette(\"Blues\", 10)\n",
    "rocket_color_pal = sns.color_palette(\"rocket\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"hdbind-rp-molformer\": (\"HDBind+MolFormer\", rocket_color_pal[0]),\n",
    "    \"hdbind-rp-ecfp-1024-1\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "#     \"hdbind-rp-ecfp-1024-2\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "#     \"hdbind-rp-ecfp-1024-4\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "#     \"hdbind-rp-ecfp-2048-1\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "#     \"hdbind-rp-ecfp-2048-2\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "#     \"hdbind-rp-ecfp-2048-4\": (\"HDBind+ECFP-1024-1\", rocket_color_pal[2]),\n",
    "    \"molehd-bpe\": (\"MoleHD-BPE\", rocket_color_pal[4]),\n",
    "#     \"smiles-pe.atomwise.0\": rocket_color_pal[2],\n",
    "#     \"smiles-pe.bpe.0\": rocket_color_pal[4],   \n",
    "# \"selfies.atomwise\": green_color_pal[4],\n",
    "#     \"ecfp\": green_color_pal[6],\n",
    "#     \"rp\": green_color_pal[8],\n",
    "#     \"rf\": blue_color_pal[4],\n",
    "#     \"mlp\": blue_color_pal[7],\n",
    "#     \"HDC-MLP\": green_color_pal[9],\n",
    "#     \"HDC-RF\": green_color_pal[9],\n",
    "#     \"Vina\": \"salmon\",\n",
    "}\n",
    "\n",
    "\n",
    "# model_order_list = [\n",
    "#     (\"hdbind-rp-molformer\", )\n",
    "#     (\"smiles-pe.atomwise.0\", \"MoleHD-Atomw.\"),\n",
    "#     (\"smiles-pe.bpe.0\", \"MoleHD-BPE\"),\n",
    "#     (\"smiles-pe.ngram.1\", \"SMILES uni-gram\"),\n",
    "#     (\"selfies.atomwise\", \"HDBind-SELFIES\"),\n",
    "#     (\"selfies.selfies-charwise\", \"SELFIES uni-gram\"),\n",
    "#     (\"ecfp\", \"HDBind-ECFP\"),\n",
    "#     (\"rp\", \"HDBind-ECFP+RP\"),\n",
    "#     (\"rf\", \"RF\"),\n",
    "#     (\"mlp\", \"MLP\"),\n",
    "#     (\"Vina\", \"Vina\")\n",
    "# ]\n",
    "\n",
    "\n",
    "# model_name_dict = {\n",
    "#     \"hdbind-rp-molformer\": \"HDBind+MolFormer\"\n",
    "#     \"smiles-pe.atomwise.0\": \"MoleHD-Atomw.\", \n",
    "#     \"smiles-pe.bpe.0\": \"MoleHD-BPE\",\n",
    "#     \"smiles-pe.ngram.1\": \"SMILES uni-gram\",\n",
    "#     \"selfies.atomwise\": \"HDBind-SELFIES\",\n",
    "#     \"selfies.selfies-charwise\": \"SELFIES uni-gram\",\n",
    "#     \"ecfp\": \"HDBind-ECFP\",\n",
    "#     \"rp\": \"HDBind-RPFP\",\n",
    "#     \"rf\": \"RF\",\n",
    "#     \"mlp\": \"MLP\",\n",
    "#     \"Vina\": \"Vina\",\n",
    "#     \"HDC-MLP\": \"HDC-MLP\",\n",
    "#     \"HDC-RF\": \"HDC-RF\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIT-PCBA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export CONDA_ROOT=/usr/workspace/jones289/anaconda3-power #installed using the LC installer script\n",
    "export PATH=$CONDA_ROOT/bin:$PATH\n",
    "export CONDA_EXE=$CONDA_ROOT/bin/conda\n",
    "ml load gcc/11\n",
    "export PYTHONPATH=$PWD:$PYTHONPATH\n",
    "source activate /usr/workspace/jones289/anaconda3-power/envs/opence-1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def compute_metrics(y_pred, y_score, y_true, p):\n",
    "    \n",
    "    return {\"precision\": precision_score(y_pred=y_pred, y_true=y_true, zero_division=0),\n",
    "            \"recall\": recall_score(y_pred=y_pred, y_true=y_true),\n",
    "           \"f1\": f1_score(y_pred=y_pred, y_true=y_true, zero_division=0),\n",
    "            \"enrich\": float(compute_enrichment_factor(scores=y_score, labels=y_true, n_percent=p)),            \n",
    "           \"roc\": roc_auc_score(y_score=y_score, y_true=y_true)\n",
    "           }\n",
    "\n",
    "\n",
    "def aggregate_results(dataset, range_limit=10, multistep_initial_p=None, \n",
    "                      multistep_p_list=None, \n",
    "                      multistep_sklearn_model=None):\n",
    "    \n",
    "\n",
    "    model_metric_dict = {\"model\": [], \"enrich\": [], \"p\":[], \"train_time\":[], \"test_time\": [], \"target\": [],\n",
    "                        \"seed\": [], \"recall\": [], \"precision\": [], \"f1\": [], \n",
    "                        }\n",
    "\n",
    "    for model_name, model_tup in tqdm(model_dict.items(), total=len(model_dict), position=0):\n",
    "                        \n",
    "\n",
    "        data_path_list = list(data_p.glob(f\"{model_name}.{dataset}*.pkl\"))\n",
    "        print(f\"{model_name}-{dataset}: {len(model_tup)}\\t {len(data_path_list)}\")\n",
    "\n",
    "        \n",
    "\n",
    "        for path in tqdm(data_path_list, total=len(data_path_list), position=1):\n",
    "\n",
    "            \n",
    "            with open(path, \"rb\") as handle:\n",
    "                model_data_dict = pickle.load(handle)\n",
    "            \n",
    "            target = path.name.split(\".\")[1]\n",
    "            \n",
    "            print(target, path, model_name)\n",
    "\n",
    "\n",
    "#             import ipdb\n",
    "#             ipdb.set_trace()\n",
    "            for seed in range(len(model_data_dict['trials'])):\n",
    "                trial_dict = model_data_dict['trials'][seed]\n",
    "            \n",
    "                if \"x_train\" in trial_dict.keys():\n",
    "                    del model_data_dict['trials'][seed]['x_train']\n",
    "                \n",
    "                if \"x_test\" in trial_dict.keys():\n",
    "                    del model_data_dict['trials'][seed]['x_test']\n",
    "                    \n",
    "                with open(path, \"wb\") as handle:\n",
    "                    pickle.dump(model_data_dict, handle)\n",
    "                \n",
    "                with open(path, \"rb\") as handle:\n",
    "                    model_data_dict = pickle.load(handle)\n",
    "                    \n",
    "                    \n",
    "                y_test = model_data_dict[\"y_test\"]\n",
    "#                 x_test = model_data_dict[\"x_test\"]\n",
    "                actives_database = sum(y_test)\n",
    "                database_size = y_test.shape[0]\n",
    "                \n",
    "#                 eta = None \n",
    "                \n",
    "\n",
    "#                 if model in [\"rf\", \"mlp\"]:\n",
    "\n",
    "\n",
    "#                     for p in [.01, .1]:\n",
    "\n",
    "#                         sklearn_model = model_data_dict[seed][\"model\"]\n",
    "\n",
    "#                         y_score = sklearn_model.predict_proba(x_test)[:, 1]            \n",
    "#                         y_pred = sklearn_model.predict(x_test)\n",
    "#                         enrich = compute_enrichment_factor(sample_scores=y_score, \n",
    "#                                                 sample_labels=y_test,\n",
    "#                                                 n_percent=p, \n",
    "#                                                 actives_database=actives_database, \n",
    "#                                                 database_size=database_size)\n",
    "\n",
    "                \n",
    "#                         metrics = compute_metrics(y_pred=y_pred, y_score=y_score, y_true=y_test)\n",
    "                \n",
    "#                         model_metric_dict[\"model\"].append(model)\n",
    "#                         model_metric_dict[\"target\"].append(target)\n",
    "#                         model_metric_dict[\"enrich\"].append(enrich)\n",
    "#                         model_metric_dict[\"p\"].append(p)\n",
    "#                         model_metric_dict[\"seed\"].append(seed)\n",
    "#                         model_metric_dict[\"precision\"].append(metrics[\"precision\"])\n",
    "#                         model_metric_dict[\"recall\"].append(metrics[\"recall\"])\n",
    "#                         model_metric_dict[\"f1\"].append(metrics[\"f1\"])\n",
    "# #                         model_metric_dict[\"roc\"].append(metrics[\"roc\"])\n",
    "\n",
    "                        \n",
    "#                 elif model.lower() in [\"hdc-rf\", \"hdc-mlp\"]:\n",
    "\n",
    "#                     sklearn_result_file = Path(f\"{data_p}/{dataset.replace('-', '_')}.{target}.{multistep_sklearn_model}.None.{ngram_order}.pkl\")\n",
    "\n",
    "#                     with open(sklearn_result_file, \"rb\") as handle:\n",
    "#                         sklearn_result_dict = pickle.load(handle)\n",
    "\n",
    "#                     target_test_hv_path = f\"{hd_cache_dir}/{target}/test_dataset_hv.pth\"\n",
    "\n",
    "#                     hv_test = torch.load(target_test_hv_path, map_location=\"cpu\")\n",
    "#                     hdc_conf_scores = model_data_dict[seed][\"model\"].compute_confidence(hv_test)\n",
    "\n",
    "# #                     # filter the data\n",
    "#                     idxs = np.flip(np.argsort(hdc_conf_scores.squeeze().cpu().numpy(), kind=\"stable\"))\n",
    "\n",
    "                    \n",
    "#                     sample_n = int(multistep_initial_p * y_test.shape[0])\n",
    "#                     samp_idxs = idxs[:sample_n]\n",
    "                    \n",
    "\n",
    "#                     x_test_samp = x_test[samp_idxs]\n",
    "#                     y_test_samp = y_test[samp_idxs]\n",
    "                                            \n",
    "    \n",
    "#                     for p in multistep_p_list:\n",
    "\n",
    "#                         sklearn_model = sklearn_result_dict[seed][\"model\"]\n",
    "            \n",
    "#                         sklearn_scores_samp = sklearn_model.predict_proba(x_test_samp)[:, 1]                        \n",
    "            \n",
    "#                         y_pred = sklearn_model.predict(x_test_samp)\n",
    "#                         enrich = compute_enrichment_factor(sample_scores=sklearn_scores_samp, \n",
    "#                                                 sample_labels=y_test_samp,\n",
    "#                                                 n_percent=p, \n",
    "#                                                 actives_database=actives_database, \n",
    "#                                                 database_size=database_size)\n",
    "\n",
    "\n",
    "#                         metrics = compute_metrics(y_pred=y_pred, y_score=sklearn_scores_samp, y_true=y_test_samp)\n",
    "                \n",
    "#                         model_metric_dict[\"model\"].append(model)\n",
    "#                         model_metric_dict[\"target\"].append(target)\n",
    "#                         model_metric_dict[\"enrich\"].append(enrich)\n",
    "#                         model_metric_dict[\"p\"].append(round(multistep_initial_p * p, 2))\n",
    "#                         model_metric_dict[\"seed\"].append(seed)\n",
    "#                         model_metric_dict[\"precision\"].append(metrics[\"precision\"])\n",
    "#                         model_metric_dict[\"recall\"].append(metrics[\"recall\"])\n",
    "#                         model_metric_dict[\"f1\"].append(metrics[\"f1\"])\n",
    "# #                         model_metric_dict[\"roc\"].append(metrics[\"roc\"])\n",
    "                    \n",
    "\n",
    "#                 else:                  \n",
    "                    \n",
    "#                 target_test_hv_path = f\"{hd_cache_dir}/{target}/test_dataset_hv.pth\"\n",
    "                \n",
    "#                 hv_test = torch.load(target_test_hv_path, map_location=\"cpu\")\n",
    "#                 hdc_conf_scores = model_data_dict[seed][\"model\"].compute_confidence(hv_test)\n",
    "                hdc_conf_scores = trial_dict[\"eta\"]\n",
    "                for p in [.01, .1]:\n",
    "\n",
    "                    try:\n",
    "                        metrics = compute_metrics(y_pred=trial_dict[\"y_pred\"], \n",
    "                                                  y_score=hdc_conf_scores, \n",
    "                                                  y_true=model_data_dict[\"y_test\"],\n",
    "                                                 p=p)\n",
    "\n",
    "    #                     model_metric_dict[\"model\"].append(model)\n",
    "                        model_metric_dict[\"target\"].append(target)\n",
    "    #                     model_metric_dict[\"enrich-1\"].append(metrics[\"enrich-1\"])\n",
    "    #                     model_metric_dict[\"enrich-10\"].append(metrics[\"enrich-10\"])\n",
    "#                         import pdb\n",
    "#                         pdb.set_trace()\n",
    "                        model_metric_dict[\"test_time\"].append(trial_dict[\"test_time\"])\n",
    "                        model_metric_dict[\"enrich\"].append(metrics[\"enrich\"])\n",
    "                        model_metric_dict[\"p\"].append(p)\n",
    "                        model_metric_dict[\"seed\"].append(seed)\n",
    "                        model_metric_dict[\"precision\"].append(metrics[\"precision\"])\n",
    "                        model_metric_dict[\"recall\"].append(metrics[\"recall\"])\n",
    "                        model_metric_dict[\"f1\"].append(metrics[\"f1\"])\n",
    "    #                     model_metric_dict[\"roc\"].append(metrics[\"roc\"])\n",
    "                        model_metric_dict[\"model\"].append(model_name)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "#                 #TODO: implement this \n",
    "#                 model_metric_dict[\"train_time\"].append(train_time)\n",
    "\n",
    "    return model_metric_dict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VINA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def docking_main(nrows=None):\n",
    "    root_p = Path(\"/p/lustre2/ahashare/zhang30/LIT-PCBA-Data/\")\n",
    "\n",
    "    path_list = [path for path in root_p.glob(\"*-actives.csv\")]\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for idx, path in tqdm(enumerate(path_list), total=len(path_list)):\n",
    "        print(idx, path)\n",
    "        \n",
    "#         '''\n",
    "        target = path.name.split(\".\")[0].split(\"-\")[0]\n",
    "        print(target, idx+1, path)\n",
    "        \n",
    "        \n",
    "\n",
    "        merged_df = None\n",
    "        merged_df_path = Path(f\"./lit_pcba_docking_analysis/{target}.csv\")\n",
    "        \n",
    "        if not merged_df_path.exists():\n",
    "            # can use the set of smiles in each result file\n",
    "            \n",
    "            # todo (10/18/23): is this valid?\n",
    "            result_pkl = Path(f\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/124/lit_pcba.{target}.ecfp.atomwise.0.pkl\")        \n",
    "\n",
    "\n",
    "            target_train_smiles_list = []\n",
    "            target_test_smiles_list = []\n",
    "\n",
    "            with open(result_pkl, \"rb\") as handle:\n",
    "\n",
    "                data = pickle.load(handle)\n",
    "\n",
    "                target_train_smiles_list = data[\"smiles_train\"]\n",
    "                target_test_smiles_list = data[\"smiles_test\"]\n",
    "\n",
    "                print(f\"total of {len(target_train_smiles_list)} in training set, total of {len(target_test_smiles_list)} in testing set.\")\n",
    "\n",
    "            df_cols = ['file', ' scores/1', ' ligName']\n",
    "            active_df = pd.read_csv(root_p / Path(f\"{target}-actives.csv.clean\"), sep=\",\", usecols=df_cols, nrows=nrows)\n",
    "            active_df['y_true'] = [1] * len(active_df)\n",
    "\n",
    "\n",
    "\n",
    "            inactive_df = pd.read_csv(root_p / Path(f\"{target}-inactives.csv.clean\"), sep=\",\", usecols=df_cols, nrows=nrows)\n",
    "            inactive_df['y_true'] = [0] * len(inactive_df)\n",
    "\n",
    "            target_df = pd.concat([active_df, inactive_df])\n",
    "            # this will search over all of the docking results for each target, across each of the multiple protein models\n",
    "\n",
    "            active_smiles_df = pd.read_csv(f\"/p/vast1/jones289/lit_pcba/{target}/actives.smi\", delim_whitespace=True, header=None)\n",
    "            inactive_smiles_df = pd.read_csv(f\"/p/vast1/jones289/lit_pcba/{target}/inactives.smi\", delim_whitespace=True, header=None)\n",
    "            target_smiles_df = pd.concat([active_smiles_df, inactive_smiles_df])\n",
    "\n",
    "\n",
    "\n",
    "            top_pose_target_df = target_df.groupby([' ligName'], as_index=False)[[' ligName', ' scores/1', 'y_true']].min()\n",
    "\n",
    "\n",
    "        \n",
    "            merged_df_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            merged_df = pd.merge(top_pose_target_df, target_smiles_df, left_on=\" ligName\", right_on=1)\n",
    "            merged_df = merged_df[merged_df.apply(lambda x: x[0] in target_test_smiles_list, axis=1)]\n",
    "            merged_df['target'] = [target] * len(merged_df)\n",
    "            merged_df.to_csv(merged_df_path, index=False)\n",
    "        else:\n",
    "            merged_df = pd.read_csv(merged_df_path)\n",
    "            \n",
    "            if 'target' not in merged_df.columns:\n",
    "                merged_df['target'] = [target] * len(merged_df)\n",
    "                merged_df.to_csv(merged_df_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "        df_list.append(merged_df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_metric_df():\n",
    "    model_metric_dict = aggregate_results(dataset=\"lit-pcba\",multistep_p_list=[.05, .5],  \n",
    "                                          multistep_sklearn_model=\"rf\", \n",
    "                                         multistep_initial_p=.2)\n",
    "\n",
    "    model_metric_df = pd.DataFrame({key: value for key, value in model_metric_dict.items() if key not in [\"train_time\", \"test_time\", \"precision\", \"recall\", \"f1\"]})\n",
    "\n",
    "    # model_metric_df\n",
    "\n",
    "    #######\n",
    "    vina_result = docking_main(nrows=None)\n",
    "    vina_result\n",
    "    ###########\n",
    "\n",
    "    docking_dict = {\"enrich\": [], \"p\": [], \"model\": [], \"target\": []}\n",
    "\n",
    "    vina_enrich_list = []\n",
    "    target_list = []\n",
    "    vina_col=' scores/1'\n",
    "    for target, target_df in vina_result.groupby(\"target\"):\n",
    "\n",
    "        for p in [.1, .01]:\n",
    "            enrich = compute_enrichment_factor(scores=np.abs(target_df[vina_col]), \n",
    "                                      labels=target_df[\"y_true\"], \n",
    "                                      n_percent=p)\n",
    "\n",
    "            docking_dict[\"enrich\"].append(float(enrich))\n",
    "            docking_dict[\"p\"].append(p)\n",
    "            docking_dict[\"model\"].append(\"Vina\")\n",
    "            docking_dict[\"target\"].append(target)\n",
    "\n",
    "\n",
    "    ##################\n",
    "    model_metric_df = pd.concat([model_metric_df, pd.DataFrame(docking_dict)])\n",
    "    \n",
    "    # Backup the calculation\n",
    "    model_metric_df.to_csv(\"fixed_litpcba_model_metric_df.csv\")\n",
    "    \n",
    "    return model_metric_df    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_metric_df = compute_model_metric_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metric_df[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_metric_df).groupby([\"model\", \"target\", \"p\"])[\"enrich\"].describe().to_csv(\"summary_ef_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_plot(enrich_1_df, enrich_10_df):\n",
    "    \n",
    "    enrich_f, enrich_ax = plt.subplots(2,1, figsize=(12,10), sharex=True, sharey=False)\n",
    "    enrich_ax = enrich_ax.flatten()\n",
    "    enrich_1_ax, enrich_10_ax = enrich_ax[0], enrich_ax[1]\n",
    "    \n",
    "#     sns.boxplot(data=enrich_1_df, x=\"model\", y=\"enrich\", ax=enrich_1_ax, palette=color_dict)\n",
    "    sns.swarmplot(data=enrich_1_df, x=\"model\", y=\"enrich\", ax=enrich_1_ax)\n",
    "    enrich_1_ax.set_title(\"(a) LIT-PCBA Enrichment at 1\\%\", fontdict={\"fontsize\": 18})\n",
    "    enrich_1_ax.set_xlabel(\"\")\n",
    "    enrich_1_ax.set_ylabel(\"\")\n",
    "    enrich_1_ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "\n",
    "    enrich_1_ax.set_ylabel(\"EF\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "#     sns.boxplot(data=enrich_10_df, x=\"model\", y=\"enrich\", ax=enrich_10_ax, palette=color_dict)\n",
    "    sns.swarmplot(data=enrich_10_df, x=\"model\", y=\"enrich\", ax=enrich_10_ax)\n",
    "\n",
    "    enrich_10_ax.set_title(\"(b) LIT-PCBA Enrichment at 10\\%\", fontdict={\"fontsize\": 18})\n",
    "    enrich_10_ax.set_xlabel(\"\")\n",
    "    enrich_10_ax.set_ylabel(\"\")\n",
    "    enrich_10_ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "    labels = [item.get_text() for item in enrich_10_ax.get_xticklabels()]\n",
    "#     labels = [model_name_dict[x.get_text()] for x in enrich_10_ax.get_xticklabels()]\n",
    "#     labels[-1] = combo_model_name\n",
    "#     enrich_10_ax.set_xticklabels(labels)\n",
    "    enrich_ax[0].set_ylabel(\"EF\")\n",
    "    enrich_ax[1].set_ylabel(\"EF\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # enrich_10_f.savefig(\"enrich_10.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    # enrich_10_f\n",
    "    \n",
    "    enrich_f.savefig(\"lit-pcba-enrich.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "#     return enrich_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average over the random seeds dimension for each combo of MODEL X TARGET X P\n",
    "grp_df = (model_metric_df).groupby([\"model\", \"target\", \"p\"])[\"enrich\"].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df[grp_df[\"p\"] == .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df[grp_df[\"p\"] == .01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_box_plot(enrich_1_df=grp_df[grp_df[\"p\"] == .01], \n",
    "          enrich_10_df=grp_df[grp_df[\"p\"] == .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- should add the average EF (median?) to the labels on the horizontal axis\n",
    "- scale the point size by standard deviation?\n",
    "- standard scale the molformer embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtra_reactivator",
   "language": "python",
   "name": "dtra_reactivator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
