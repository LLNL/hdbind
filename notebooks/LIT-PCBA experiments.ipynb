{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIT-PCBA \n",
    "this dataset is a proposed \"more difficult\" test than DUD-E. I used a 75/25 (sklearn default) stratified split. The dataset is heavily imbalanced, like DUD-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "sys.path.insert(0, \"/g/g13/jones289/workspace/hd-cuda-master/hdpy\")\n",
    "# print(sys.path)\n",
    "import hdpy\n",
    "import hdpy.ecfp_hd\n",
    "from hdpy.analysis import load_pkl\n",
    "from hdpy.metrics import compute_enrichment_factor\n",
    "\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.rc('axes', labelsize=17)\n",
    "plt.rc('figure', titlesize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = Path(\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125\")\n",
    "# data_p = Path(\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/124\")\n",
    "# data_p = Path(\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/4\")\n",
    "# data_p = Path(\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/0\")\n",
    "# data_p = Path(\"/usr/WS1/jones289/hd-cuda-master/hdpy/hdpy/before_rng_results/before_rng_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_color_pal = sns.color_palette(\"Greens\", 10)\n",
    "blue_color_pal = sns.color_palette(\"Blues\", 10)\n",
    "rocket_color_pal = sns.color_palette(\"rocket\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "#     \"smiles-pe.atomwise.0\": rocket_color_pal[2],\n",
    "#     \"smiles-pe.bpe.0\": rocket_color_pal[4],   \n",
    "# \"selfies.atomwise\": green_color_pal[4],\n",
    "    \"ecfp\": green_color_pal[6],\n",
    "#     \"rp\": green_color_pal[8],\n",
    "    \"rf\": blue_color_pal[4],\n",
    "#     \"mlp\": blue_color_pal[7],\n",
    "#     \"HDC-MLP\": green_color_pal[9],\n",
    "    \"HDC-RF\": green_color_pal[9],\n",
    "    \"Vina\": \"salmon\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "marker_dict = {\n",
    "#     \"smiles-pe\": \"+\",\n",
    "    \"smiles-pe.atomwise.0\": \"+\",\n",
    "    \"smiles-pe.bpe.0\": \"+\",   \n",
    "#     \"smiles-pe.ngram.1\": \"+\",\n",
    "    \"selfies.atomwise\": \"*\",\n",
    "#     \"selfies.selfies-charwise\": \"*\",\n",
    "    \"ecfp\": \"+\",\n",
    "    \"rp\": \"+\",\n",
    "    \"rf\": \"^\",\n",
    "#     \"openhd\": \"*\",\n",
    "    \"mlp\": \"+\",\n",
    "#     \"Vina\": \"+\",\n",
    "}\n",
    "\n",
    "\n",
    "model_order_list = [\n",
    "    (\"smiles-pe.atomwise.0\", \"MoleHD-Atomw.\"),\n",
    "    (\"smiles-pe.bpe.0\", \"MoleHD-BPE\"),\n",
    "#     (\"smiles-pe.ngram.1\", \"SMILES uni-gram\"),\n",
    "    (\"selfies.atomwise\", \"HDBind-SELFIES\"),\n",
    "#     (\"selfies.selfies-charwise\", \"SELFIES uni-gram\"),\n",
    "    (\"ecfp\", \"HDBind-ECFP\"),\n",
    "    (\"rp\", \"HDBind-ECFP+RP\"),\n",
    "    (\"rf\", \"RF\"),\n",
    "    (\"mlp\", \"MLP\"),\n",
    "#     (\"Vina\", \"Vina\")\n",
    "]\n",
    "\n",
    "\n",
    "model_name_dict = {\n",
    "    \"smiles-pe.atomwise.0\": \"MoleHD-Atomw.\", \n",
    "    \"smiles-pe.bpe.0\": \"MoleHD-BPE\",\n",
    "#     \"smiles-pe.ngram.1\": \"SMILES uni-gram\",\n",
    "    \"selfies.atomwise\": \"HDBind-SELFIES\",\n",
    "#     \"selfies.selfies-charwise\": \"SELFIES uni-gram\",\n",
    "    \"ecfp\": \"HDBind-ECFP\",\n",
    "    \"rp\": \"HDBind-RPFP\",\n",
    "    \"rf\": \"RF\",\n",
    "    \"mlp\": \"MLP\",\n",
    "    \"Vina\": \"Vina\",\n",
    "    \"HDC-MLP\": \"HDC-MLP\",\n",
    "    \"HDC-RF\": \"HDC-RF\"\n",
    "}\n",
    "\n",
    "\n",
    "linestyle_dict = {\n",
    "    \"smiles-pe.atomwise.0\": \"-\", \n",
    "    \"smiles-pe.bpe.0\": \":\",\n",
    "    \"selfies.None\": \"-\",\n",
    "    \"ecfp\": \":\",\n",
    "    \"rp\": \"-.\",\n",
    "    \"rf\": \"-\",\n",
    "    \"mlp\": \":\",\n",
    "    \"Vina\": \"-\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# def compute_average_enrichment(pkl_path, model, range_limit=10):\n",
    "    \n",
    "#     enrich_1_list = [] \n",
    "#     enrich_10_list = []\n",
    "\n",
    "#     with open(pkl_path, \"rb\") as handle:\n",
    "#         data = pickle.load(handle) # lets just use one seed\n",
    "            \n",
    "#         for seed in range(range_limit):\n",
    "            \n",
    "#             y_true = None\n",
    "#             if isinstance(data[seed][\"y_true\"], np.ndarray):\n",
    "#                 y_true = data[seed][\"y_true\"]\n",
    "#             else:\n",
    "#                 y_true = np.concatenate(data[seed][\"y_true\"]) \n",
    "\n",
    "    \n",
    "#             eta = None \n",
    "    \n",
    "#             if model in [\"rf\", \"mlp\"]:\n",
    "#                 eta= data[seed][\"model\"].predict_proba(data[\"x_test\"])[:,1]\n",
    "#             else:\n",
    "#                 eta = np.array(data[seed][\"eta\"])\n",
    "\n",
    "#             enrich_1 = compute_enrichment_factor(sample_scores=eta, sample_labels=y_true, n_percent=.01)\n",
    "#             enrich_1_list.append(enrich_1)\n",
    "                \n",
    "#             enrich_10 = compute_enrichment_factor(sample_scores=eta, sample_labels=y_true, n_percent=.1)\n",
    "#             enrich_10_list.append(enrich_10)\n",
    "            \n",
    "    \n",
    "#     return np.mean(enrich_1_list), np.std(enrich_1_list), np.mean(enrich_10_list), np.std(enrich_10_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def compute_average_train_time(pkl_path):\n",
    "#     print(pkl_path)\n",
    "#     train_time_list = []\n",
    "    \n",
    "\n",
    "#     with open(pkl_path, \"rb\") as handle:\n",
    "#         data = pickle.load(handle) # lets just use one seed\n",
    "        \n",
    "#         target = pkl_path.name.split(\".\")[2]\n",
    "#         split = pkl_path.name.split(\".\")[1]\n",
    "#         model = pkl_path.name.split(\".\")[3]\n",
    "\n",
    "#         range_limit = 10\n",
    "    \n",
    "#         if \"openhd\" in model:\n",
    "#             range_limit=1\n",
    "#         for seed in range(range_limit):\n",
    "\n",
    "#             train_time  = data[seed][\"train_time\"]\n",
    "\n",
    "#             if \"openhd\" in model:\n",
    "#                 pass\n",
    "            \n",
    "#             elif \"mlp\" in model:\n",
    "#                 train_time = train_time / 10 # trained MLP for max 10 epochs...check that it hits that but assume it didn't converge that quickly\n",
    "#             elif \"rf\" in model:\n",
    "#                 train_time = train_time # there's no concept of epoch in a random forest\n",
    "#             else:\n",
    "#                 train_time = train_time/10 # this should catch all HD models which were trained for 10 epochs per seed\n",
    "            \n",
    "#             train_time_list.append(train_time)\n",
    "            \n",
    "            \n",
    "    \n",
    "#     return np.mean(train_time_list)\n",
    "\n",
    "\n",
    "\n",
    "# def compute_average_inference_time(pkl_path):\n",
    "    \n",
    "#     test_time_list = []\n",
    "    \n",
    "\n",
    "#     with open(pkl_path, \"rb\") as handle:\n",
    "#         data = pickle.load(handle) # lets just use one seed\n",
    "        \n",
    "\n",
    "#         range_limit = 10\n",
    "    \n",
    "#         if \"openhd\" in str(pkl_path):\n",
    "#             range_limit=1\n",
    "#         for seed in range(range_limit):\n",
    "\n",
    "#             test_time  = data[seed][\"test_time\"]\n",
    "\n",
    "#             test_time_list.append(test_time)\n",
    "            \n",
    "            \n",
    "    \n",
    "#     return np.mean(test_time_list)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIT-PCBA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(dataset, range_limit=10, multistep_initial_p=None, \n",
    "                      multistep_p_list=None, \n",
    "                      multistep_sklearn_model=None):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model_metric_dict = {\"model\": [], \"enrich\": [], \"p\":[], \"train_time\":[], \"test_time\": [], \"target\": [],\n",
    "                        \"seed\": []}\n",
    "    tokenizer=\"atomwise\"\n",
    "    ngram_order=0\n",
    "\n",
    "    for model, color in color_dict.items():\n",
    "                        \n",
    "        metric_list = []\n",
    "        encode_time_list = []\n",
    "        train_time_list = []\n",
    "        test_time_list = []\n",
    "        train_size_list = []\n",
    "        test_size_list = []\n",
    "        target_size_list = []\n",
    "        eta_list = []\n",
    "\n",
    "        \n",
    "        if model not in [\"HDC-MLP\", \"HDC-RF\"]:\n",
    "            data_path_list = list(data_p.glob(f\"{dataset.replace('-','_')}*.{model}*pkl\"))\n",
    "#             print(model)\n",
    "        elif model in [\"HDC-MLP\", \"HDC-RF\"]:\n",
    "#             print(f\"{model}: multistep filter to be implemented\")\n",
    "            data_path_list = list(data_p.glob(f\"{dataset.replace('-','_')}*.ecfp*pkl\"))\n",
    "        \n",
    "\n",
    "        for path in tqdm(data_path_list, total=len(data_path_list)):\n",
    "            \n",
    "            print(path, model)\n",
    "            \n",
    "            with open(path, \"rb\") as handle:\n",
    "                model_data_dict = pickle.load(handle)\n",
    "\n",
    "            \n",
    "            target = path.name.split(\".\")[1]\n",
    "            \n",
    "            if target != \"KAT2A\":\n",
    "                continue\n",
    "            \n",
    "            hd_cache_dir = f\"/p/lustre2/jones289/hd_cache/125/ecfp/{dataset}/random\"\n",
    "\n",
    "            \n",
    "                \n",
    "            for seed in range(range_limit):\n",
    "                                \n",
    "                y_true = model_data_dict[\"y_test\"]\n",
    "                eta = None \n",
    "\n",
    "                if model in [\"rf\", \"mlp\"]:\n",
    "\n",
    "#                     eta = model_data_dict[seed][\"model\"].predict_proba(model_data_dict[\"x_test\"])[:,1]\n",
    "                    eta = model_data_dict[seed][\"eta\"][:, 1]\n",
    "\n",
    "                    for p in [.01, .1]:\n",
    "\n",
    "                        enrich = compute_enrichment_factor(sample_scores=eta, sample_labels=y_true, n_percent=p)\n",
    "\n",
    "                        model_metric_dict[\"model\"].append(model)\n",
    "                        model_metric_dict[\"target\"].append(target)\n",
    "                        model_metric_dict[\"enrich\"].append(enrich)\n",
    "                        model_metric_dict[\"p\"].append(p)\n",
    "                        model_metric_dict[\"seed\"].append(seed)\n",
    "\n",
    "                        \n",
    "                elif model.lower() in [\"hdc-rf\", \"hdc-mlp\"]:\n",
    "                    import pdb \n",
    "                    pdb.set_trace()\n",
    "                    sklearn_result_file = Path(f\"{data_p}/{dataset.replace('-', '_')}.{target}.{multistep_sklearn_model}.None.{ngram_order}.pkl\")\n",
    "\n",
    "                    with open(sklearn_result_file, \"rb\") as handle:\n",
    "                        sklearn_result_dict = pickle.load(handle)                        \n",
    "                    \n",
    "                    \n",
    "                    if isinstance(model_data_dict[seed][\"y_true\"], np.ndarray):\n",
    "                        y_true = model_data_dict[seed][\"y_true\"]\n",
    "                    else:\n",
    "                        y_true = np.concatenate(model_data_dict[seed][\"y_true\"])\n",
    "                    \n",
    "                    \n",
    "                    target_test_hv_path = f\"{hd_cache_dir}/{target}/test_dataset_hv.pth\"\n",
    "                \n",
    "                \n",
    "                    hv_test = torch.load(target_test_hv_path, map_location=\"cpu\")\n",
    "                    \n",
    "\n",
    "                    hdc_model = model_data_dict[seed][\"model\"]\n",
    "#                     hdc_model = hdc_model.to(\"cpu\")\n",
    "#                     hv_test = hv_test.cpu()\n",
    "                    hdc_conf_scores = hdc_model.compute_confidence(hv_test)\n",
    "#                     hdc_conf_scores = torch.from_numpy(model_data_dict[seed][\"eta\"])\n",
    "                                            \n",
    "#                     # filter the data\n",
    "                    values , idxs = torch.sort(hdc_conf_scores.squeeze().cpu(), descending=True)\n",
    "\n",
    "                    sample_n = int(np.ceil(multistep_initial_p * y_true.shape[0]))\n",
    "\n",
    "                    hd_actives = sum(y_true[idxs[:sample_n]])\n",
    "\n",
    "                    actives_database = sum(y_true)\n",
    "#                     # rescore at 10% \n",
    "                            \n",
    "#                     # rescore at 1%\n",
    "                                            \n",
    "                    for p in multistep_p_list:\n",
    "\n",
    "#                         sklearn_model = sklearn_result_dict[seed][\"model\"]\n",
    "#                         # get the indexes of the top initial-p% of compounds ranked by HDC\n",
    "                        samp_idxs = (idxs[:sample_n]).numpy()\n",
    "\n",
    "#                         # take result of filtering from HDC\n",
    "                        x_test_samp = model_data_dict[\"x_test\"][samp_idxs]\n",
    "                        y_true_samp = y_true[samp_idxs]\n",
    "\n",
    "#                         sklearn_scores = sklearn_model.predict_proba(x_test_samp)[:, 1]\n",
    "                        sklearn_scores = sklearn_result_dict[seed][\"eta\"][:, 1]\n",
    "            \n",
    "            \n",
    "                        enrich = compute_enrichment_factor(sample_scores=sklearn_scores, \n",
    "                                                sample_labels=y_true_samp,\n",
    "                                                n_percent=p, \n",
    "                                                actives_database=sum(y_true), \n",
    "                                                database_size=y_true.shape[0])\n",
    "\n",
    "                        model_metric_dict[\"model\"].append(model)\n",
    "                        model_metric_dict[\"target\"].append(target)\n",
    "                        model_metric_dict[\"enrich\"].append(enrich)\n",
    "                        model_metric_dict[\"p\"].append(p)\n",
    "                        model_metric_dict[\"seed\"].append(seed)\n",
    "\n",
    "                else:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    \n",
    "#                     print(f\"{model} not implemented yet\")\n",
    "                    if isinstance(model_data_dict[seed][\"y_true\"], np.ndarray):\n",
    "                        y_true = model_data_dict[seed][\"y_true\"]\n",
    "                    else:\n",
    "                        y_true = np.concatenate(model_data_dict[seed][\"y_true\"])\n",
    "                    \n",
    "                    target_test_hv_path = f\"{hd_cache_dir}/{target}/test_dataset_hv.pth\"\n",
    "                \n",
    "                \n",
    "                    hv_test = torch.load(target_test_hv_path, map_location=\"cpu\")\n",
    "                    hdc_model = model_data_dict[seed][\"model\"]\n",
    "#                     hdc_model = hdc_model.to(\"cpu\")\n",
    "#                     hv_test = hv_test.cpu()\n",
    "                    hdc_conf_scores = hdc_model.compute_confidence(hv_test)\n",
    "#                     hdc_conf_scores = model_data_dict[seed][\"eta\"]\n",
    "\n",
    "                    \n",
    "                    for p in [.01, .1]:\n",
    "\n",
    "                        enrich = compute_enrichment_factor(sample_scores=hdc_conf_scores, \n",
    "                                                sample_labels=y_true,\n",
    "                                                n_percent=p, \n",
    "                                                actives_database=sum(y_true), \n",
    "                                                database_size=y_true.shape[0])\n",
    "\n",
    "                        model_metric_dict[\"model\"].append(model)\n",
    "                        model_metric_dict[\"target\"].append(target)\n",
    "                        model_metric_dict[\"enrich\"].append(enrich)\n",
    "                        model_metric_dict[\"p\"].append(p)\n",
    "                        model_metric_dict[\"seed\"].append(seed)\n",
    "                    \n",
    "\n",
    "    return model_metric_dict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.ALDH1.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/15 [00:11<02:39, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.GBA.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [00:19<02:00,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.ESR1_ago.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [00:19<01:02,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.VDR.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [00:30<01:22,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.PKM2.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 5/15 [00:38<01:14,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.MAPK1.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 6/15 [00:40<00:50,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.FEN1.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 7/15 [00:50<00:57,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.PPARG.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 8/15 [00:51<00:35,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.MTORC1.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 9/15 [00:52<00:23,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.ESR1_ant.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 10/15 [00:53<00:14,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.OPRK1.ecfp.atomwise.0.pkl ecfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 11/15 [01:01<00:17,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/125/lit_pcba.KAT2A.ecfp.atomwise.0.pkl ecfp\n",
      "> \u001b[0;32m/tmp/ipykernel_1784358/1422841380.py\u001b[0m(142)\u001b[0;36maggregate_results\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    140 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    141 \u001b[0;31m\u001b[0;31m#                     print(f\"{model} not implemented yet\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 142 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    143 \u001b[0;31m                        \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    144 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/tmp/ipykernel_1784358/1422841380.py\u001b[0m(143)\u001b[0;36maggregate_results\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    141 \u001b[0;31m\u001b[0;31m#                     print(f\"{model} not implemented yet\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    142 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 143 \u001b[0;31m                        \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    144 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    145 \u001b[0;31m                        \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/tmp/ipykernel_1784358/1422841380.py\u001b[0m(147)\u001b[0;36maggregate_results\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    145 \u001b[0;31m                        \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    146 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 147 \u001b[0;31m                    \u001b[0mtarget_test_hv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{hd_cache_dir}/{target}/test_dataset_hv.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    148 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    149 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/tmp/ipykernel_1784358/1422841380.py\u001b[0m(150)\u001b[0;36maggregate_results\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    148 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    149 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 150 \u001b[0;31m                    \u001b[0mhv_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test_hv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m                    \u001b[0mhdc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m\u001b[0;31m#                     hdc_model = hdc_model.to(\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/tmp/ipykernel_1784358/1422841380.py\u001b[0m(151)\u001b[0;36maggregate_results\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    149 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m                    \u001b[0mhv_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test_hv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 151 \u001b[0;31m                    \u001b[0mhdc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m\u001b[0;31m#                     hdc_model = hdc_model.to(\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    153 \u001b[0;31m\u001b[0;31m#                     hv_test = hv_test.cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> hv_test\n",
      "tensor([[-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.]])\n",
      "ipdb> hv_test.flatten().sum()\n",
      "tensor(-342578.)\n",
      "ipdb> target\n",
      "'KAT2A'\n",
      "ipdb> target\n",
      "'KAT2A'\n",
      "ipdb> model\n",
      "'ecfp'\n",
      "ipdb> target_test_hv_path\n",
      "'/p/lustre2/jones289/hd_cache/125/ecfp/lit-pcba/random/KAT2A/test_dataset_hv.pth'\n",
      "ipdb> hv_test[0, :]\n",
      "tensor([-1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "         1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
      "         1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
      "        -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
      "         1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
      "         1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
      "        -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "        -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
      "        -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
      "        -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
      "         1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
      "         1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "         1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
      "         1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "         1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
      "         1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
      "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
      "         1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
      "        -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
      "        -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,\n",
      "         1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
      "        -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
      "        -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
      "        -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
      "         1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "        -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "         1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "        -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
      "         1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
      "         1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
      "        -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
      "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,\n",
      "         1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
      "         1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "         1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "         1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
      "        -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
      "        -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
      "        -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
      "        -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,\n",
      "         1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "        -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "        -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
      "         1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
      "        -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
      "         1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,\n",
      "        -1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "         1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
      "         1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
      "        -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "         1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
      "         1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "        -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
      "         1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "        -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "         1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
      "        -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
      "        -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
      "         1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
      "         1., -1., -1., -1., -1.,  1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> hv_test[0, :].sum()\n",
      "tensor(-8.)\n",
      "ipdb> hv_test[0, :].sum()\n",
      "tensor(-8.)\n",
      "ipdb> hv_test[1, :].sum()\n",
      "tensor(-34.)\n",
      "ipdb> hv_test[2, :].sum()\n",
      "tensor(-6.)\n"
     ]
    }
   ],
   "source": [
    "model_metric_dict = aggregate_results(dataset=\"lit-pcba\",multistep_p_list=[.01, .1],  \n",
    "                                      multistep_sklearn_model=\"rf\", \n",
    "                                     multistep_initial_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([len(x) for x in model_metric_dict.values()])\n",
    "# print([x for x in model_metric_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'smiles-pe.ngram.2', 'smiles-pe.ngram.3', 'rf', 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metric_df = pd.DataFrame({key: value for key, value in model_metric_dict.items() if key not in [\"train_time\", \"test_time\"]})\n",
    "\n",
    "model_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VINA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def docking_main(nrows=None):\n",
    "    root_p = Path(\"/p/lustre2/ahashare/zhang30/LIT-PCBA-Data/\")\n",
    "\n",
    "#     color_dict.update({\"Vina\": \"salmon\"})\n",
    "\n",
    "    path_list = [path for path in root_p.glob(\"*-actives.csv\")]\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for idx, path in tqdm(enumerate(path_list), total=len(path_list)):\n",
    "        print(idx, path)\n",
    "        \n",
    "#         '''\n",
    "        target = path.name.split(\".\")[0].split(\"-\")[0]\n",
    "        print(target, idx+1, path)\n",
    "        \n",
    "        \n",
    "\n",
    "        merged_df = None\n",
    "        merged_df_path = Path(f\"./lit_pcba_docking_analysis/{target}.csv\")\n",
    "        \n",
    "        if not merged_df_path.exists():\n",
    "            # can use the set of smiles in each result file\n",
    "            result_pkl = Path(f\"/g/g13/jones289/workspace/hd-cuda-master/hdpy/hdpy/results/124/lit_pcba.{target}.ecfp.atomwise.0.pkl\")        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            target_train_smiles_list = []\n",
    "            target_test_smiles_list = []\n",
    "\n",
    "            with open(result_pkl, \"rb\") as handle:\n",
    "\n",
    "                data = pickle.load(handle)\n",
    "\n",
    "                target_train_smiles_list = data[\"smiles_train\"]\n",
    "                target_test_smiles_list = data[\"smiles_test\"]\n",
    "\n",
    "                print(f\"total of {len(target_train_smiles_list)} in training set, total of {len(target_test_smiles_list)} in testing set.\")\n",
    "\n",
    "            df_cols = ['file', ' scores/1', ' ligName']\n",
    "            active_df = pd.read_csv(root_p / Path(f\"{target}-actives.csv\"), sep=\",\", usecols=df_cols, nrows=nrows)\n",
    "            active_df['y_true'] = [1] * len(active_df)\n",
    "\n",
    "\n",
    "\n",
    "            inactive_df = pd.read_csv(root_p / Path(f\"{target}-inactives.csv\"), sep=\",\", usecols=df_cols, nrows=nrows)\n",
    "            inactive_df['y_true'] = [0] * len(inactive_df)\n",
    "\n",
    "            target_df = pd.concat([active_df, inactive_df])\n",
    "            # this will search over all of the docking results for each target, across each of the multiple protein models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            active_smiles_df = pd.read_csv(f\"/p/vast1/jones289/lit_pcba/{target}/actives.smi\", delim_whitespace=True, header=None)\n",
    "            inactive_smiles_df = pd.read_csv(f\"/p/vast1/jones289/lit_pcba/{target}/inactives.smi\", delim_whitespace=True, header=None)\n",
    "            target_smiles_df = pd.concat([active_smiles_df, inactive_smiles_df])\n",
    "\n",
    "\n",
    "\n",
    "            top_pose_target_df = target_df.groupby([' ligName'], as_index=False)[[' ligName', ' scores/1', 'y_true']].min()\n",
    "\n",
    "\n",
    "        \n",
    "            merged_df_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            merged_df = pd.merge(top_pose_target_df, target_smiles_df, left_on=\" ligName\", right_on=1)\n",
    "            merged_df = merged_df[merged_df.apply(lambda x: x[0] in target_test_smiles_list, axis=1)]\n",
    "            merged_df['target'] = [target] * len(merged_df)\n",
    "            merged_df.to_csv(merged_df_path, index=False)\n",
    "        else:\n",
    "            merged_df = pd.read_csv(merged_df_path)\n",
    "            \n",
    "            if 'target' not in merged_df.columns:\n",
    "                merged_df['target'] = [target] * len(merged_df)\n",
    "                merged_df.to_csv(merged_df_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "        df_list.append(merged_df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "#######\n",
    "vina_result = docking_main(nrows=None)\n",
    "vina_result\n",
    "###########\n",
    "\n",
    "docking_dict = {\"enrich\": [], \"p\": [], \"model\": [], \"target\": []}\n",
    "\n",
    "vina_enrich_list = []\n",
    "target_list = []\n",
    "vina_col=' scores/1'\n",
    "for target, target_df in vina_result.groupby(\"target\"):\n",
    "    \n",
    "    for p in [.1, .01]:\n",
    "        enrich = compute_enrichment_factor(sample_scores=np.abs(target_df[vina_col]), \n",
    "                                  sample_labels=target_df[\"y_true\"], \n",
    "                                  n_percent=p)\n",
    "        \n",
    "        docking_dict[\"enrich\"].append(float(enrich))\n",
    "        docking_dict[\"p\"].append(p)\n",
    "        docking_dict[\"model\"].append(\"Vina\")\n",
    "        docking_dict[\"target\"].append(target)\n",
    "    \n",
    "\n",
    "##################\n",
    "model_metric_df = pd.concat([model_metric_df, pd.DataFrame(docking_dict)])\n",
    "\n",
    "model_metric_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metric_df[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(enrich_1_df, enrich_10_df):\n",
    "\n",
    "    enrich_f, enrich_ax = plt.subplots(2,1, figsize=(12,10), sharex=True, sharey=False)\n",
    "    enrich_ax = enrich_ax.flatten()\n",
    "    enrich_1_ax, enrich_10_ax = enrich_ax[0], enrich_ax[1]\n",
    "    \n",
    "    sns.boxplot(data=enrich_1_df, x=\"model\", y=\"enrich\", ax=enrich_1_ax, palette=color_dict)\n",
    "    enrich_1_ax.set_title(\"(a) LIT-PCBA Enrichment at 1\\%\", fontdict={\"fontsize\": 18})\n",
    "    enrich_1_ax.set_xlabel(\"\")\n",
    "    enrich_1_ax.set_ylabel(\"\")\n",
    "    enrich_1_ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "\n",
    "    enrich_1_ax.set_ylabel(\"EF\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    sns.boxplot(data=enrich_10_df, x=\"model\", y=\"enrich\", ax=enrich_10_ax, palette=color_dict)\n",
    "    enrich_10_ax.set_title(\"(b) LIT-PCBA Enrichment at 10\\%\", fontdict={\"fontsize\": 18})\n",
    "    enrich_10_ax.set_xlabel(\"\")\n",
    "    enrich_10_ax.set_ylabel(\"\")\n",
    "    enrich_10_ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "    labels = [item.get_text() for item in enrich_10_ax.get_xticklabels()]\n",
    "    labels = [model_name_dict[x.get_text()] for x in enrich_10_ax.get_xticklabels()]\n",
    "#     labels[-1] = combo_model_name\n",
    "    enrich_10_ax.set_xticklabels(labels)\n",
    "    enrich_ax[0].set_ylabel(\"EF\")\n",
    "    enrich_ax[0].set_ylabel(\"EF\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # enrich_10_f.savefig(\"enrich_10.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    # enrich_10_f\n",
    "    \n",
    "    enrich_f.savefig(\"lit-pcba-enrich.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "#     return enrich_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(enrich_1_df=model_metric_df, enrich_10_df=model_metric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_metric_df.groupby(\"model\").describe()[['enrich-1-mean', 'enrich-10-mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_dict = {group_name: group_df for group_name, group_df in model_metric_df.groupby('model')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name in group_dict.keys():\n",
    "#     print(f\"{group_name}-{group_dict[group_name]['train_time'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name in group_dict.keys():\n",
    "#     print(f\"{group_name}-{group_dict[group_name]['test_time'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1,1, figsize=(14,8))\n",
    "\n",
    "# g = sns.boxplot(data=model_metric_df[model_metric_df[\"model\"] != \"Vina\"], x=\"model\", y=\"train_time\", ax=ax, palette=color_dict)\n",
    "# ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "# g.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1,1, figsize=(14,8))\n",
    "# \n",
    "# g = sns.boxplot(data=model_metric_df[model_metric_df[\"model\"] != \"Vina\"], x=\"model\", y=\"test_time\", ax=ax, palette=color_dict)\n",
    "# ax.tick_params(axis=\"x\", labelrotation=22.5)\n",
    "# g.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HD_env",
   "language": "python",
   "name": "hd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
