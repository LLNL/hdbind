# -*- coding: utf-8 -*-
"""HD-Graph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zR-LhpvyGkehnSwDetLq5DC5NgutI_cJ
"""

# !pip3 install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu113.html -q

import torch
from torch_geometric.data import Data
from torch_geometric.datasets import TUDataset, Planetoid
import cupy as cp
import numpy as np
from itertools import permutations
import datetime
from torch_geometric.transforms import NormalizeFeatures
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import f1_score

# dataset = Planetoid(root='./data/Cora', name='Cora', split="full")
dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer', split="full")
# dataset = Planetoid(root='./data/Pubmed', name='Pubmed', split="full")



print(dataset, "length of dataset: ", len(dataset))
data = dataset[0]
print("Data: ", data)
print("# of node features: ", data.num_node_features)
print(f'Number of training nodes: {data.train_mask.sum()}')
print(f'Number of val nodes: {data.val_mask.sum()}')
print(f'Number of test nodes: {data.test_mask.sum()}')
print("Data Undirected?: ", data.is_undirected())
print(data.x[0].to_sparse().indices())
print(data.x[0].to_sparse().values())

arr = torch.flatten(data.x)
max_feature_val = arr.max()
min_feature_val = arr.min()
print('[Feature Val] Max: {0} and Min: {1}'.format(max_feature_val, min_feature_val))


def trace(*args):
    print(datetime.datetime.now().strftime('%H:%M:%S')+' '+' '.join(map(str,args)))

def gen_hv(D, F, non_linear=False):
    hv_mat = []
    for _ in range(F):
        if non_linear == False:
            random_hv = cp.random.choice([1, -1], size=D, p=[0.5, 0.5]).astype(cp.int32)
        else:
            random_hv = cp.random.normal(size=D, dtype=cp.float32)  # mu=1, sigma=0
        hv_mat.append(random_hv)
    return cp.array(hv_mat)

def gen_lvs(D: int, Q: int):
    base = np.ones(D)
    base[:D//2] = -1.0
    l0 = np.random.permutation(base)
    levels = list()
    for i in range(Q+1):
        flip = int(int(i/float(Q) * D) / 2)
        li = np.copy(l0)
        li[:flip] = l0[:flip] * -1
        levels.append(list(li))
    return levels


def flip_based_lvs(D: int, totalFeatures: int, flip_factor: int, non_linear=False):

    nFlip = int(D//flip_factor)

    mu = 0
    sigma = 1
    bases = np.random.normal(mu, sigma, D)
    if non_linear == False:
        bases = 2*(bases >= 0) - 1  # polarize

    import copy
    generated_hvs = [copy.copy(bases)]

    for ii in range(totalFeatures-1):
        # id_hvs.append(np.random.normal(mu, sigma, D))
        idx_to_flip = np.random.randint(0, D, size=nFlip)
        bases[idx_to_flip] *= (-1)
        generated_hvs.append(copy.copy(bases))

    return generated_hvs

def polarize(arr, enable):
    if enable == True:
        arr = torch.sign(arr)
        arr[arr==0] = 1
    return arr

def rho(hv, num):
    if hv.dim() == 1:
        return torch.roll(hv, num)
    else:
        return torch.roll(hv, num, dims=1)

def normalize(x, x_test=None, normalizer='l2'):
    if normalizer == 'l2':
        from sklearn.preprocessing import Normalizer
        scaler = Normalizer(norm='l2').fit(x)
        x_norm = scaler.transform(x)
        if x_test is None:
            return x_norm, None
        else:
            return x_norm, scaler.transform(x_test)

    elif normalizer == 'minmax': # Here, we assume knowing the global max & min
        from sklearn.preprocessing import MinMaxScaler
        if x_test is None:
            x_data = x
        else:
            x_data = np.concatenate((x, x_test), axis=0)

        scaler = MinMaxScaler().fit(x_data)
        x_norm = scaler.transform(x)
        if x_test is None:
            return x_norm, None
        else:
            return x_norm, scaler.transform(x_test)

    raise NotImplemented


dim=8192
binarize=False
enable_non_linear=False
refine_threshold = 0.5   # higher, more memorization
train_iter_num = 20
memoryhv_iter_num = 20
lr = 1
q = 32  # q = 1 this generates two levels, [0, 1]

if max_feature_val != 1:
    trace('Normalizing node features...')
    node_features, _ = normalize(data.x, normalizer='l2')
    node_features = torch.Tensor(node_features)

    arr = torch.flatten(node_features)
    max_feature_val = arr.max()
    min_feature_val = arr.min()
    print('[After Normalization: Feature Val] Max: {0} and Min: {1}'.format(max_feature_val, min_feature_val))
else:
    node_features = data.x
# node_features = data.x

assert(node_features.shape == data.x.shape)




trace('Feature HV Generation')
feature_hvs = gen_hv(D=dim, F=data.num_node_features, non_linear=enable_non_linear)
feature_hvs = torch.Tensor(feature_hvs).cuda().to(torch.float32)

trace('Level HV Generation')
level_hvs = gen_lvs(D=dim, Q=q)
level_hvs = torch.Tensor(level_hvs).cuda().to(torch.float32)
all_feature_arange = torch.arange(data.num_node_features)

trace('Node Property HV Generation')
boo0 = torch.Tensor(cp.random.choice([1, -1], size=dim, p=[0.5, 0.5]).astype(cp.float32)).cuda()
boo1 = torch.Tensor(cp.random.choice([1, -1], size=dim, p=[0.5, 0.5]).astype(cp.float32)).cuda()
boo2 = torch.Tensor(cp.random.choice([1, -1], size=dim, p=[0.5, 0.5]).astype(cp.float32)).cuda()

# xxxxx = level_hvs
# xxxxx = torch.Tensor(flip_based_lvs(D=dim, totalFeatures=3, flip_factor=2, non_linear=False)).cuda()
# boo0 = xxxxx[0]
# boo1 = xxxxx[1]
# boo2 = xxxxx[2]

trace('Node HV Generation')
node_hvs = torch.zeros((data.num_nodes, dim), dtype=torch.float32, device='cuda')
for node_idx, node in enumerate(node_features):
    feature_exist = node.to_sparse().coalesce().indices().squeeze()
    if feature_exist.ndim == 0:
        node_hvs[node_idx] = feature_hvs[feature_exist]
    else:
        node_hvs[node_idx] = torch.sum(feature_hvs[feature_exist], dim=0).view(-1)
    
    # lv_indices = (node.to_sparse().coalesce().values() * q).squeeze().to(torch.int64)
    # node_hvs[node_idx] = torch.sum(torch.mul(feature_hvs[feature_exist], level_hvs[lv_indices]), dim=0).view(-1)
node_hvs = polarize(node_hvs, binarize)


# trace('LSA')
# lsa_dim = 50
# svd = TruncatedSVD(n_components=lsa_dim)
# lsa_result = svd.fit_transform(node_features.cpu().data.numpy())
# print(lsa_result.shape)
# print(node_features.shape)

# # # Non Linear Encoding; cos_ degrades acc
# # lsa_feature_hvs = gen_hv(D=dim, F=lsa_dim, non_linear=True)
# # lsa_feature_hvs = torch.Tensor(lsa_feature_hvs).cuda().to(torch.float32)
# # print(lsa_feature_hvs.shape)
# # node_hvs = torch.sign(torch.matmul(torch.Tensor(lsa_result).cuda(), lsa_feature_hvs))

# #RFF (node_feature => RBF: bad acc; node_feature=>LSA=>RBF)
# from sklearn.kernel_approximation import RBFSampler
# rbf_feature = RBFSampler(gamma=0.3, n_components=dim)
# node_hvs = torch.Tensor(np.sign(rbf_feature.fit_transform(lsa_result))).cuda().to(torch.float32)

# print(node_hvs.shape)


trace('Memory HV Generation')
memory_hvs = torch.zeros((data.num_nodes, dim), dtype=torch.float32).cuda()
edge_coordinates = torch.transpose(data.edge_index, 0, 1).cpu().detach().numpy()
for (h, t) in edge_coordinates:
    memory_hvs[t] += node_hvs[h]
memory_hvs = polarize(memory_hvs, binarize)


trace('Memory LV2 Generation')
memory_lv2_hvs = torch.zeros((data.num_nodes, dim), dtype=torch.float32).cuda()
for (h, t) in edge_coordinates:
    memory_lv2_hvs[t] += memory_hvs[h]
memory_lv2_hvs = rho(memory_lv2_hvs, -1)
memory_lv2_hvs = polarize(memory_lv2_hvs, binarize)


# tmp = []
# for (h, t) in edge_coordinates:
#     found = 0
#     for (h2, t2) in edge_coordinates:
#         if t2 == h and ((t2, h2) != (h, t)):
#             found = 1
#             tmp.append((h2, h, t))
#     if found == 0:
#         tmp.append((-1, h, t))
# print(tmp)
# for (hh, h, t) in tmp:
#     if hh != -1:
#         memory_hvs[t] += (node_hvs[h] + node_hvs[hh])
#     else:
#         memory_hvs[t] += node_hvs[h]
# memory_hvs = polarize(memory_hvs, binarize)


# trace('Iterative Memory HV Refinement')
# for _ in range(memoryhv_iter_num):
#     refine=0
#     for (h, t) in edge_coordinates:
#         if torch.dot(node_hvs[h], memory_hvs[t])/(torch.linalg.norm(node_hvs[h])*torch.linalg.norm(memory_hvs[t])) < refine_threshold:
#             memory_hvs[t] += node_hvs[h]
#             refine +=1
#     print(refine)
    
#     refine=0
#     for (h, t) in edge_coordinates:
#         if torch.dot(memory_hvs[h], memory_lv2_hvs[t])/(torch.linalg.norm(memory_hvs[h])*torch.linalg.norm(memory_lv2_hvs[t])) < refine_threshold:
#             memory_lv2_hvs[t] += memory_hvs[h]
#             refine +=1
#     print(refine)
#     memory_hvs = polarize(memory_hvs, binarize)
#     memory_lv2_hvs = polarize(memory_lv2_hvs, binarize)

# Note: This takes too long...
# lst = list(range(data.num_nodes))
# pair_order_list = permutations(lst, 2)
# edge_coordinates_list = edge_coordinates.tolist()
# iter_num = 10
# for _ in range(memoryhv_iter_num):
#     for (h, t) in pair_order_list:
#         if [h, t] in edge_coordinates_list:
#             if torch.dot(node_hvs[h], memory_hvs[t]) < 0.5*dim:  # 0.5: threshold
#                 memory_hvs[t] += node_hvs[h]  
#         else:
#             if torch.dot(node_hvs[h], memory_hvs[t]) > 0.5*dim:  # 0.5: threshold
#                 memory_hvs[t] -= node_hvs[h]
#     memory_hvs = polarize(memory_hvs, binarize)

num_classes = len(torch.unique(data.y))
class_hvs = torch.zeros((num_classes, dim), dtype=torch.float32).cuda()
y_true = data.y[data.test_mask]
total = int(data.test_mask.sum())

trace('Single Pass Training')
# single pass training
for node_idx, node_y in enumerate(data.y):
    if data.train_mask[node_idx] == True:
        # class_hvs[node_y] += node_hvs[node_idx]
        # class_hvs[node_y] += node_hvs[node_idx] * memory_hvs[node_idx]
        # class_hvs[node_y] += node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx]
        class_hvs[node_y] += node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2
class_hvs = polarize(class_hvs, binarize)


trace('Testing => num_classes:', num_classes, "total test size:", total)
pred_list = []
for node_idx, node_y in enumerate(data.y):
    if data.test_mask[node_idx] == True:
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx])
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx])
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx])
        sim_vec = torch.matmul(class_hvs, node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2)
        if not binarize:
            denom = torch.linalg.norm(class_hvs, dim=1, ord=2)
            sim_vec = sim_vec/denom
        pred_y = torch.argmax(sim_vec)
        pred_list.append(int(pred_y))
print(f1_score(y_true, pred_list, average='micro'))


trace('Iterative Training')
# iterative training
for _ in range(20):
    refine=0
    for node_idx, node_y in enumerate(data.y):
        if data.train_mask[node_idx] == True:
            # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx])
            # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx])
            # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx])
            sim_vec = torch.matmul(class_hvs, node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2)
            if not binarize:
                denom = torch.linalg.norm(class_hvs, dim=1, ord=2)
                sim_vec = sim_vec/denom
            pred_y = torch.argmax(sim_vec)
            if pred_y != node_y:
                refine+=1
                # class_hvs[node_y] += lr * node_hvs[node_idx]
                # class_hvs[pred_y] -= lr * node_hvs[node_idx]
                # class_hvs[node_y] += lr * node_hvs[node_idx] * memory_hvs[node_idx] 
                # class_hvs[pred_y] -= lr * node_hvs[node_idx] * memory_hvs[node_idx] 
                # class_hvs[node_y] += lr * node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx]
                # class_hvs[pred_y] -= lr * node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx]
                class_hvs[node_y] += lr * (node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2)
                class_hvs[pred_y] -= lr * (node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2)
                class_hvs = polarize(class_hvs, binarize)
    print(refine, end =" ")
print("")

print(node_hvs.max())
print(memory_hvs.max())
print(memory_lv2_hvs.max())
print(class_hvs.max())

trace('Testing => num_classes:', num_classes, "total test size:", total)
score = 0
pred_list = []
for node_idx, node_y in enumerate(data.y):
    if data.test_mask[node_idx] == True:
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx])
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx])
        # sim_vec = torch.matmul(class_hvs, node_hvs[node_idx] * memory_hvs[node_idx] * memory_lv2_hvs[node_idx])
        sim_vec = torch.matmul(class_hvs, node_hvs[node_idx]*boo0 + memory_hvs[node_idx]*boo1 + memory_lv2_hvs[node_idx]*boo2)
        if not binarize:
            denom = torch.linalg.norm(class_hvs, dim=1, ord=2)
            sim_vec = sim_vec/denom
        pred_y = torch.argmax(sim_vec)
        pred_list.append(int(pred_y))
print(f1_score(y_true, pred_list, average='micro'))  # micro is same as acc


